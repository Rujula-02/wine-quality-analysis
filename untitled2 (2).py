# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sn--3rWyJNGxUOZueV_YDecu_Kwx-ci3
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import TomekLinks
from sklearn.utils.class_weight import compute_class_weight

import pandas as pd

from google.colab import files

uploaded = files.upload()

data = pd.read_csv("WineQT.csv")

data.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Display basic information about the dataset
print(data.info())
print(data['Id'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6, 4))
sns.countplot(x='quality', data=data, palette='viridis')
plt.title('Quality Distribution in the Wine Dataset')
plt.xlabel('Quality')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(6, 6))
quality_counts = data['quality'].value_counts()
plt.pie(quality_counts, labels=quality_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Quality Proportion in the Wine Dataset')
plt.show()

from sklearn.model_selection import train_test_split

# Split the dataset into features and target variable
X = data.drop('quality', axis=1)
y = data['quality']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from imblearn.under_sampling import RandomUnderSampler
import pandas as pd

rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(X_train, y_train)

print("Original class distribution:\n", y_train.value_counts())
print("\nAfter Random Under Sampling:\n", pd.Series(y_rus).value_counts())

from imblearn.over_sampling import RandomOverSampler
import pandas as pd

ros = RandomOverSampler(random_state=42)
X_ros, y_ros = ros.fit_resample(X_train, y_train)

print("Original class distribution:\n", y_train.value_counts())
print("\nAfter Random Over Sampling:\n", pd.Series(y_ros).value_counts())

from imblearn.over_sampling import SMOTE
import pandas as pd

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_train, y_train)

print("Original class distribution:\n", y_train.value_counts())
print("\nAfter SMOTE:\n", pd.Series(y_smote).value_counts())

from imblearn.under_sampling import TomekLinks
import pandas as pd

tl = TomekLinks()
X_tl, y_tl = tl.fit_resample(X_train, y_train)

print("Original class distribution:\n", y_train.value_counts())
print("\nAfter Tomek Links:\n", pd.Series(y_tl).value_counts())

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

print("Class Weights:\n", class_weights_dict)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import numpy as np
import warnings
from sklearn.exceptions import UndefinedMetricWarning

def evaluate_model(X_train, y_train, X_test, y_test, class_weights=None):
    # Train the model
    model = RandomForestClassifier(class_weight=class_weights, random_state=42)
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Predicted probabilities
    try:
        y_prob = model.predict_proba(X_test)

        # Filter classes present in y_test only
        present_classes = np.unique(y_test)
        class_indices = [list(model.classes_).index(cls) for cls in present_classes]
        y_prob_filtered = y_prob[:, class_indices]

        # AUC computation
        auc_score = roc_auc_score(y_test, y_prob_filtered, multi_class='ovr', labels=present_classes)
    except Exception as e:
        auc_score = "AUC computation failed: " + str(e)

    # Suppress undefined metric warnings
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
        print("Classification Report:\n", classification_report(y_test, y_pred, zero_division=0))

    print("AUC (OvR):", auc_score)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Load data
data = pd.read_csv('WineQT.csv')

# Split features and target
X = data.drop(['quality', 'Id'], axis=1)
y = data['quality']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

def evaluate_model(X_train_sampled, y_train_sampled, X_test, y_test, sampler_name="Model", class_weights=None):
    # Pass class_weight only if provided
    if class_weights is not None:
        model = LogisticRegression(max_iter=1000, class_weight=class_weights)
    else:
        model = LogisticRegression(max_iter=1000)

    model.fit(X_train_sampled, y_train_sampled)
    y_pred = model.predict(X_test)

    try:
        y_proba = model.predict_proba(X_test)
        auc = roc_auc_score(y_test, y_proba, multi_class='ovr')
    except:
        auc = "N/A (AUC failed)"

    print(f"{sampler_name}:\n{classification_report(y_test, y_pred, zero_division=0)}")
    print(f"AUC: {auc}\n{'-'*60}\n")



# Random Undersampling
X_rus, y_rus = RandomUnderSampler(random_state=42).fit_resample(X_train, y_train)
evaluate_model(X_rus, y_rus, X_test, y_test, "Random Undersampling")

# Random Oversampling
X_ros, y_ros = RandomOverSampler(random_state=42).fit_resample(X_train, y_train)
evaluate_model(X_ros, y_ros, X_test, y_test, "Random Oversampling")

# SMOTE
X_smote, y_smote = SMOTE(random_state=42, k_neighbors=1).fit_resample(X_train, y_train)
evaluate_model(X_smote, y_smote, X_test, y_test, "SMOTE")

# Tomek Links
X_tl, y_tl = TomekLinks().fit_resample(X_train, y_train)
evaluate_model(X_tl, y_tl, X_test, y_test, "Tomek Links")

# Class Weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
weights_dict = dict(zip(np.unique(y_train), class_weights))
evaluate_model(X_train, y_train, X_test, y_test, "Class Weights", class_weights=weights_dict)